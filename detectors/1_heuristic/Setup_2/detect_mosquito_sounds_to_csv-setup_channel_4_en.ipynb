{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c568a4-96d2-45d1-ac26-cf6aa5332007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cf9e5-b965-49dd-9569-2b1b9ff89dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root=\"./input_recordings\"\n",
    "output_root=\"./preprocessed_database\"\n",
    "channel_num=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ca6f3-ef37-463c-9bdc-b1cdc3c096bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.signal import butter, filtfilt\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Supported audio file formats\n",
    "supported_formats = [\".wav\"]\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199edd1-8696-4ec0-a2e5-6ac163e1b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highpass_filter(y, sr, cutoff=250, order=4):\n",
    "    #Applies a high-pass filter to the data.\n",
    "    nyquist = 0.5 * sr\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return filtfilt(b, a, y)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d802472-337a-4bdf-acc3-0c3cb3c549a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_spectrogram_segment(segment, sr, n_fft, hop_length, threshold=12): # 15 filters well, 10 is more permissive, but also brings in negatives\n",
    "    \"\"\"\n",
    "    Analyze a single spectrogram segment.\n",
    "    \"\"\"\n",
    "    # Aggregate spectrum by taking the maximum of each frequency bin\n",
    "    #aggregated_spectrum = np.max(segment, axis=1)\n",
    "    aggregated_spectrum = np.mean(segment, axis=1)\n",
    "\n",
    "    # Frequencies calculation\n",
    "    freqs = np.fft.rfftfreq(n=n_fft, d=1.0/sr)\n",
    "\n",
    "    # Selecting the 300-1500 Hz range\n",
    "    target_indices = np.where((freqs >= 300) & (freqs <= 1500))[0]\n",
    "    target_spectrum = aggregated_spectrum[target_indices]\n",
    "\n",
    "    # Find the first local minimum\n",
    "    if target_spectrum[0] >= target_spectrum[1]:\n",
    "        local_min_indices = librosa.util.peak_pick(-target_spectrum, pre_max=3, post_max=3, pre_avg=5, post_avg=5, delta=0.3, wait=1)\n",
    "        first_local_min_index = target_indices[local_min_indices[0]] if len(local_min_indices) > 0 else target_indices[0]\n",
    "    else:\n",
    "        first_local_min_index = target_indices[0]\n",
    "\n",
    "    # Find the maximum value after the first local minimum\n",
    "    subsequent_spectrum = aggregated_spectrum[first_local_min_index:]\n",
    "    max_index = first_local_min_index + np.argmax(subsequent_spectrum)\n",
    "\n",
    "    # Find the local minimum after the maximum\n",
    "    remaining_spectrum = aggregated_spectrum[max_index:]\n",
    "    local_min_indices = librosa.util.peak_pick(-remaining_spectrum, pre_max=3, post_max=3, pre_avg=5, post_avg=5, delta=0.3, wait=1)\n",
    "    local_min_index = max_index + local_min_indices[0] if len(local_min_indices) > 0 else max_index + np.argmin(remaining_spectrum)\n",
    "\n",
    "    # Calculate decibel difference\n",
    "    max_db = aggregated_spectrum[max_index]\n",
    "    min_db = aggregated_spectrum[local_min_index]\n",
    "    db_difference = max_db - min_db\n",
    "\n",
    "    # PLOTTING ----------------------------\n",
    "    \"\"\"\n",
    "    # Visualization of aggregated spectrum and key points\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(freqs, aggregated_spectrum, label='Aggregated Spectrum')\n",
    "    plt.scatter(freqs[first_local_min_index], aggregated_spectrum[first_local_min_index], color='red', label=f'First Min: {aggregated_spectrum[first_local_min_index]:.2f}')\n",
    "    plt.scatter(freqs[max_index], aggregated_spectrum[max_index], color='green', label=f'Max: {aggregated_spectrum[max_index]:.2f}')\n",
    "    plt.scatter(freqs[local_min_index], aggregated_spectrum[local_min_index], color='blue', label=f'Post-Max Min: {aggregated_spectrum[local_min_index]:.2f}')\n",
    "    plt.legend()\n",
    "    plt.title('Aggregated Spectrum with Local Min/Max Points')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude (dB)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    if freqs[max_index] > 1500:  # No fundamental frequency in the appropriate range\n",
    "        return False\n",
    "\n",
    "    if (freqs[max_index] > 750) & (freqs[max_index] < 775):  # there is a continuous frequency (765 Hz) in every segment\n",
    "        return False\n",
    "\n",
    "    return db_difference > threshold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "# Updated filter_audio_chunk function with spectrogram segmentation\n",
    "def filter_audio_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Filter function that decides whether the given audio chunk contains a mosquito sound.\n",
    "    \"\"\"\n",
    "    data = chunk.get_array_of_samples()\n",
    "    dtype = data.typecode  # The type of the array, e.g., 'h' or 'i'\n",
    "    y = np.array(data, dtype=np.float32)  # Always convert to float32\n",
    "\n",
    "    # Normalization depending on the data type\n",
    "    if dtype == 'h':  # 16-bit integer\n",
    "        y = y / (2**15)  # Normalize between -1 and 1\n",
    "    elif dtype == 'i':  # 32-bit integer\n",
    "        y = y / (2**31)  # Normalize between -1 and 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data format: {dtype}\")\n",
    "        \n",
    "    #y = np.array(chunk.get_array_of_samples(), dtype=np.float32) / (2**15)\n",
    "    sr = chunk.frame_rate\n",
    "\n",
    "    # 100 Hz high-pass filtering\n",
    "    y = highpass_filter(y, sr)\n",
    "\n",
    "    # Spectrogram calculation after highpass filter\n",
    "    #n_fft = 512  # FFT window size for 8 kHz\n",
    "    #xhop_length = 50  # Hop size\n",
    "    n_fft = 1024  # FFT window size for 16 kHz\n",
    "    hop_length = 50  # Hop size\n",
    "    \n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    \n",
    "    # PLOTTING ----------------------------\n",
    "    \"\"\"\n",
    "    # Display spectrogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    librosa.display.specshow(S_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='linear', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram After Highpass Filter')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # -------------------------------------\n",
    "\n",
    "    mosquito_count = 0\n",
    "    \n",
    "    # Spectrogram slicing into 10 segments\n",
    "    num_segments = 10\n",
    "    segment_length = S_db.shape[1] // num_segments\n",
    "\n",
    "    raw_segment_length = len(y) // num_segments\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * segment_length\n",
    "        end = start + segment_length if i < num_segments - 1 else S_db.shape[1]\n",
    "        spectrogram_segment = S_db[:, start:end]\n",
    "\n",
    "        raw_start = i * raw_segment_length\n",
    "        raw_end = raw_start + raw_segment_length if i < num_segments - 1 else len(y)\n",
    "        y_segment_max = np.max(np.abs(y[raw_start:raw_end]))\n",
    "\n",
    "        # Analyze the spectrogram segment\n",
    "        # discard the signal if it's too weak, don't analyze it\n",
    "        #if (y_segment_max>0.02) and analyze_spectrogram_segment(spectrogram_segment, sr, n_fft, hop_length):\n",
    "        if (y_segment_max>0.01) and (y_segment_max<0.12) and analyze_spectrogram_segment(spectrogram_segment, sr, n_fft, hop_length):\n",
    "            mosquito_count += 1\n",
    "\n",
    "    return mosquito_count >= 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a292e-7d03-4c4c-975c-b77c30705a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_chunks(input_file, channel_num):\n",
    "    \"\"\"\n",
    "    Read audio file, determine number of channels, split into 1-second chunks with 0.5-second overlap.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "        # Determine the number of channels\n",
    "        num_channels = audio.channels\n",
    "        print(f\"File: {input_file}, Number of channels: {num_channels}\")\n",
    "        if num_channels!=channel_num:\n",
    "            print(\"incorrect channel number\")\n",
    "            return  [], [], num_channels\n",
    "\n",
    "        # Separate all channels\n",
    "        separated_channels = audio.split_to_mono()\n",
    "\n",
    "        for channel_index, channel_audio in enumerate(separated_channels):\n",
    "\n",
    "            # Convert to 8kHz\n",
    "            #channel_audio = channel_audio.set_frame_rate(8000)\n",
    "            channel_audio = channel_audio.set_frame_rate(16000)\n",
    "\n",
    "            duration_ms = len(channel_audio)\n",
    "\n",
    "            # x-second window, y-second step\n",
    "            window_size = 1000  # in milliseconds\n",
    "            step_size = 500     # in milliseconds\n",
    "\n",
    "            for start_ms in range(0, duration_ms - window_size + 1, step_size):\n",
    "                end_ms = start_ms + window_size\n",
    "                chunk = channel_audio[start_ms:end_ms]\n",
    "                chunks.append(chunk)\n",
    "\n",
    "                # Record metadata\n",
    "                metadata.append({\n",
    "                    \"file\": os.path.basename(input_file),\n",
    "                    \"channel\": channel_index + 1,\n",
    "                    \"start_ms\": start_ms,\n",
    "                    \"end_ms\": end_ms\n",
    "                })\n",
    "            #break # channel\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during splitting: {input_file} - {e}\")\n",
    "\n",
    "    return chunks, metadata, num_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17fd49-ab90-4314-9fb3-4b6684dcb996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "\n",
    "model_silero_vad = load_silero_vad()\n",
    "\n",
    "def filter_speech(chunk_16000):\n",
    "\n",
    "    data = chunk_16000.get_array_of_samples()\n",
    "    dtype = data.typecode  # The type of the array, e.g., 'h' or 'i'\n",
    "    y = np.array(data, dtype=np.float32)  # Always convert to float32\n",
    "\n",
    "    # Normalization depending on the data type\n",
    "    if dtype == 'h':  # 16-bit integer\n",
    "        y = y / (2**15)  # Normalize between -1 and 1\n",
    "    elif dtype == 'i':  # 32-bit integer\n",
    "        y = y / (2**31)  # Normalize between -1 and 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data format: {dtype}\")\n",
    "        \n",
    "    #y = np.array(chunk_8000.get_array_of_samples(), dtype=np.float32) / (2**15)\n",
    "    #sr = chunk_8000.frame_rate\n",
    "\n",
    "    # 8 kHz -> 16 kHz conversion\n",
    "    #chunk_16000 = librosa.resample(y, orig_sr=8000, target_sr=16000)\n",
    "    #chunk_16000 = torch.tensor(chunk_16000, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "      y,\n",
    "      model_silero_vad,\n",
    "      return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",
    "    )\n",
    "    \n",
    "    if len(speech_timestamps):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91239919-56f8-4f37-8508-776877fbd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anal_chunks(chunks, output_dir, base_filename, metadata, b_save=False):\n",
    "    \"\"\"\n",
    "    Save chunks to the specified folder with numbering.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    not_selected_dir = os.path.join(os.path.dirname(output_dir), os.path.basename(output_dir) + \"_not_selected\")\n",
    "    os.makedirs(not_selected_dir, exist_ok=True)\n",
    "\n",
    "    speech_dir = os.path.join(os.path.dirname(output_dir), os.path.basename(output_dir) + \"_speech\")\n",
    "    os.makedirs(speech_dir, exist_ok=True)\n",
    "\n",
    "    sound_idxs=[]\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        md=metadata[idx]\n",
    "\n",
    "        ch=md['channel']\n",
    "        start=str(int(md['start_ms']))\n",
    "        \n",
    "        if filter_speech(chunk):\n",
    "            if b_save:\n",
    "                chunk_speech_file = os.path.join(speech_dir, f\"{base_filename}_{ch}_{start}.wav\")            \n",
    "                #chunk.export(chunk_speech_file, format=\"wav\", parameters=[\"-ar\", \"16000\", \"-ac\", \"1\", \"-sample_fmt\", \"s16\"])\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                pass\n",
    "                #print(f\"{idx}: Saved chunk (SPEECH): {chunk_speech_file}\")\n",
    "            continue\n",
    "        \n",
    "        if filter_audio_chunk(chunk):\n",
    "            sound_idxs.append(idx)\n",
    "\n",
    "            if b_save:\n",
    "                chunk_output_file = os.path.join(output_dir, f\"{base_filename}_{ch}_{start}.wav\")\n",
    "                chunk.export(chunk_output_file, format=\"wav\", parameters=[\"-ar\", \"16000\", \"-ac\", \"1\", \"-sample_fmt\", \"s16\"])\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                pass\n",
    "                print(f\"{idx}: Saved chunk (MOSQUITO SOUND): {chunk_output_file}\")\n",
    "        else:\n",
    "            if b_save:\n",
    "                not_selected_file = os.path.join(not_selected_dir, f\"{base_filename}_{ch}_{start}.wav\")\n",
    "                #chunk.export(not_selected_file, format=\"wav\", parameters=[\"-ar\", \"16000\", \"-ac\", \"1\", \"-sample_fmt\", \"s16\"])\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                pass\n",
    "                #print(f\"{idx}: Not selected chunk: {not_selected_file}\")\n",
    "\n",
    "    return sound_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c1643-b0ce-48ce-b447-63a7a7395a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28320c86-8e40-4d36-995c-641b5ae8cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fns=glob.glob(os.path.join(input_root,\"*.wav\"))\n",
    "\n",
    "for idx, input_fn in enumerate(fns):\n",
    "\n",
    "    base_filename=os.path.basename(input_fn)\n",
    "    output_file = os.path.join(output_root, f\"{base_filename[:-4]}.csv\")\n",
    "    \n",
    "    # If the output file already exists, continue with the next file\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "\n",
    "    # Splitting and filtering\n",
    "    chunks, metadata, num_channels = split_audio_chunks(input_fn, channel_num=channel_num)\n",
    "    if len(chunks)>0:\n",
    "        sound_idxs=anal_chunks(chunks, output_root, base_filename, metadata, b_save=True)\n",
    "\n",
    "        if len(sound_idxs)>0:\n",
    "            # Create Metadata DataFrame\n",
    "            out_df = pd.DataFrame(metadata)\n",
    "            \n",
    "            # Filter based on analyzed indices\n",
    "            out_df = out_df.iloc[sound_idxs]\n",
    "            \n",
    "            # Save results to file\n",
    "            out_df.to_csv(output_file, index=False)\n",
    "        else:\n",
    "            print(\"no mosquito sound found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcba944-cb50-40e6-a0cc-deecaf101773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fc5b9-0ff4-4d6a-a258-8c5b7b12cd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Mosquito Sound Detector: Training

This repository contains the scripts for preparing the audio dataset and training two types of mosquito sound classifiers, based on the data generated by the heuristic detectors.

---

## 1. Data Preparation: Train/Validation Split

Before training, the audio segments (.wav files) must be collected and split into a formal `train` and `validation` dataset.

### Data Collection

First, collect all your 1-second audio segments into two main folders:

* **`.../input_database/mosquito/`**: Contains all positive mosquito sound segments.
* **`.../input_database/not/`**: Contains all negative (non-mosquito) sound segments.

**Positive Samples:** The positive samples can be generated using the heuristic detectors described in:
* `detectors/1_heuristic/ABUZZ/README.md`
* `detectors/1_heuristic/Setup_1/README.md`
* `detectors/1_heuristic/Setup_2/README.md`

**Negative Samples:** The negative samples folder should contain a large and diverse set of non-mosquito sounds. The script `1_training/copyRandomWavs_en.ipynb` is provided to help create this dataset by randomly sampling a specified number (`n`) of `.wav` files from a larger source database (like the ABUZZ non-mosquito set).

### Train/Validation Set Creation

The script **`1_training/b_mosquito_create_train_valid_sets_en.ipynb`** is used to split the collected data into `train` and `validation` sets.

This script does **not** split files randomly. To prevent data leakage (where segments from the same original file end up in both train and validation), it splits files based on their **prefix**. All segments from the same source recording (e.g., `Test0001_1_500.wav`, `Test0001_1_1000.wav`) are grouped and will be placed together in *either* the training or the validation set, but not both.

**Key Parameters in `b_mosquito_create_train_valid_sets_en.ipynb`:**

* `input_dir`: The root folder containing the `mosquito` and `not` subfolders.
* `output_dir`: The destination folder where the `train/mosquito`, `train/not`, `validation/mosquito`, and `validation/not` structures will be created.
* `blacklist_fn`: Path to a text file. Any file whose **prefix** contains a string from this blacklist will be **excluded** from the dataset. This is used to remove known problematic or corrupted recordings.
* `train_ratio`: The percentage of data to allocate to the training set (e.g., `0.8` for 80%). This split is applied to the list of unique prefixes.
* `class_ratios`: A dictionary to downsample classes. For example, `{'mosquito':0.35, 'not':1.0}` will only use 35% of the available mosquito files while using 100% of the 'not' files. This is useful for balancing the dataset before splitting.

---

## 2. Model Training

Once the `train` and `validation` folders are populated, you can train a classifier. This repository provides two options:

### Option A: ResNet + Self-Attention

**Script:** `b_mosquito_train_simple_classifier.ipynb`

This notebook trains a custom **ResNet-18** model from scratch, modified to include a **self-attention** layer (`b_selfattention=True`).

* It first generates temporary CSV files (`trainData.csv`, `valiData.csv`) from the folder structure to feed its data loader.
* It uses a custom `dataPreprocess.py` and `model_modified.py` (which must be in the same directory).
* Training progress and classification reports are saved to text files (e.g., `b_mosquito_simple_classifier-1_output_big.txt`).
* Checkpoints are saved in a corresponding folder.

### Option B: Audio Spectrogram Transformer (AST)

**Script:** `train_huggingface_AST_b_mosquito_25_01_21.ipynb`

This notebook fine-tunes a pre-trained **Audio Spectrogram Transformer (AST)** model from the Hugging Face library (`MIT/ast-finetuned-audioset-10-10-0.4593`).

* It uses the Hugging Face `datasets` library to load the audio files directly from the `train` and `validation` folders.
* It supports freezing the base model and training only the classification head (`b_train_just_last=True`) or the last `N` layers (`trainNlayers > 0`).
* It uses the `Trainer` class, which handles metric computation (Accuracy, F1, Precision, Recall) and saves checkpoints.
* Parameters like `everyNth` can be used to downsample the dataset if you encounter memory (RAM) issues during preprocessing.

---

## 3. Inference

After training, you can use the models to detect mosquito sounds in new audio recordings. The following scripts are provided for inference:

### Option A: ResNet + Self-Attention Inference

* **Script:** `2_inference/simple_classifier/detect_mosquito_sounds_simple_classifier.ipynb`
* **Description:** This notebook loads a trained ResNet checkpoint and processes audio files. It applies the same chunking logic as the heuristic detectors and saves the segments classified as 'mosquito'.

### Option B: AST Inference

* **Script:** `2_inference/AST/detect_mosquito_sounds_AST.ipynb`
* **Description:** This notebook loads a fine-tuned AST checkpoint and performs inference on new audio files, saving segments that meet the classification threshold.

### Combining Detections

* **Script:** `2_inference/combine_resnet_and_AST_detections.ipynb`
* **Description:** This script offers a method to create a final, high-precision dataset by combining the results from both the ResNet and AST models. It filters the outputs, keeping only the segments detected by *both* models, thereby removing false positives that one model may have caught but the other correctly rejected.